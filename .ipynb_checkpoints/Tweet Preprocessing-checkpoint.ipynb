{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1598968120527,
     "user": {
      "displayName": "Joe Netherton",
      "photoUrl": "",
      "userId": "04700075469670085746"
     },
     "user_tz": -60
    },
    "id": "lWEXrjX_kd2T",
    "outputId": "73ac7aa6-d397-4fbd-c7aa-1a44d172feaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/thesis_files\n"
     ]
    }
   ],
   "source": [
    "#Import Packages\n",
    "\n",
    "#Preprocessing\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import emoji\n",
    "from emoji import UNICODE_EMOJI\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "#Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkhd_d4vkhNZ"
   },
   "outputs": [],
   "source": [
    "#choose dataset\n",
    "\n",
    "dataset = \"large\" #large: 3 party datset, \"small\": 2 party dataset\n",
    "\n",
    "#select tweets containing key words to create topic datasets\n",
    "creating_topic_dataset = False #To create topic datset set to \"True\"\n",
    "\n",
    "topic = \"brexit\" # or \"covid\" or \"blm\"\n",
    "\n",
    "if creating_topic_dataset == True:\n",
    "  if topic == \"brexit\":\n",
    "    keywords = [\"brexit\", \"EU\", \"Leave\", \"UKIP\",\"European\", \"Union\"] #Brexit keywords\n",
    "\n",
    "  if topic == \"covid\":\n",
    "    keywords = [\"covid\", \"coronavirus\", \"pandemic\", \"covid-19\", \"corona\"] #COVID keywords\n",
    "\n",
    "  if topic == \"blm\":\n",
    "    keywords = [\"black\", \"lives\", \"matter\", \"blacklivesmatter\", \"george\", \"floyd\", \"racism\"] #BLM keywords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1598968172986,
     "user": {
      "displayName": "Joe Netherton",
      "photoUrl": "",
      "userId": "04700075469670085746"
     },
     "user_tz": -60
    },
    "id": "TGbnYNgT7W3-",
    "outputId": "13320748-1a48-415d-831d-80a2271c0ad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eojju\\Documents\\University\\Supporting Materials\\thesis_files\\Data\n"
     ]
    }
   ],
   "source": [
    "#move to directory of data file\n",
    "%cd Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1654,
     "status": "ok",
     "timestamp": 1598968182879,
     "user": {
      "displayName": "Joe Netherton",
      "photoUrl": "",
      "userId": "04700075469670085746"
     },
     "user_tz": -60
    },
    "id": "F9akPyXNkjDB",
    "outputId": "9349f1e8-720e-4b7b-c379-59648ddcba26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 89831 Tweets from  Labour party.\n",
      "Loaded 74972 Tweets from  Conservative party.\n",
      "Loaded 87385 Tweets from  SNP party.\n"
     ]
    }
   ],
   "source": [
    "#Import data from csv file\n",
    "\n",
    "data = pd.read_csv('labour_tweets.csv', lineterminator='\\n')\n",
    "print(\"Loaded\", len(data), \"Tweets from \", \"Labour party.\")\n",
    "data2 = pd.read_csv('conservative_tweets.csv', lineterminator='\\n')\n",
    "print(\"Loaded\", len(data2), \"Tweets from \", \"Conservative party.\")\n",
    "\n",
    "if dataset == \"large\": \n",
    "  data3 = pd.read_csv('snp_tweets.csv', lineterminator='\\n')\n",
    "  print(\"Loaded\", len(data3), \"Tweets from \", \"SNP party.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mo4kvI-kkn7_"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5f0ca56f83a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnew_line_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mchar_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mlabel_encoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "#Set Hyper params\n",
    "MAX_LENGTH = 290\n",
    "user_tag = \"<USER/>\"\n",
    "no_hashtags_tag = \"<NO TAGS/>\"\n",
    "other_emoji_tag = \"<OTHER_EMOJI/>\"\n",
    "emoji_tag = \"<EMOJI/>\"\n",
    "new_line_tag = \"\\n\"\n",
    "char_dict = {}\n",
    "label_encoder = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qf7BJRYnkrYn"
   },
   "outputs": [],
   "source": [
    "def split(word): \n",
    "    return [char for char in word] \n",
    "\n",
    "\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "   \n",
    "\n",
    "def emoji_check(input_string):\n",
    "  #returns True if one or more emojis in String\n",
    "  emoji_list = UNICODE_EMOJI.keys() \n",
    "  res = [ele for ele in emoji_list if(ele in input_string)] \n",
    "  if len(res) != 0:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def preprocess_string(input_string):\n",
    "\n",
    "  input_string = input_string.lower() #convert to lowercase\n",
    "  input_string = strip_accents(input_string) #remove accents from letters\n",
    "  input_string = input_string.rstrip() #remove trailing newlines: \"\\n\"\n",
    "\n",
    "  filters = [\"…\", \"\\n\", \"£\", \"$\", \"“\", \"–\", \"\\xa0\", \"”\", \"—\", \"ø\", \"\\r\"]\n",
    "  pre_string = [] #init\n",
    "  hashtags = [] #init\n",
    "  pre_string =[]\n",
    "\n",
    "  for i in filters:\n",
    "    input_string = input_string.replace(i, \"\")\n",
    "\n",
    "  input_string = re.split(\"( )\", input_string) #split the tweet by words\n",
    "\n",
    "  for word in input_string:\n",
    "    \n",
    "    word = (word.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "    if emoji_check(word) == True:\n",
    "      pre_string.append(emoji_tag)\n",
    "\n",
    "    elif \"@\" in word:\n",
    "      pre_string.append(user_tag)\n",
    "    #elif \"#\" in word:\n",
    "      #hashtags.append(word)\n",
    "\n",
    "    else:\n",
    "      pre_string += split(word)\n",
    "\n",
    "  if len(hashtags) == 0:\n",
    "    hashtags.append(no_hashtags_tag)  \n",
    "\n",
    "  return pre_string, hashtags\n",
    "\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "  #applies preprocesing to an entire corpus\n",
    "  #returns list of preprocessed tweet and list of hashtags\n",
    "  print(\"Preprocessing corpus . . .\")\n",
    "  pre_corpus = []\n",
    "  hashtags = []\n",
    "  for tweet in corpus:\n",
    "  \n",
    "    pre, tags = preprocess_string(tweet)\n",
    "    pre = np.asarray(pre)\n",
    "    tags = np.asarray(tags)\n",
    "    \n",
    "    pre_corpus.append(pre)\n",
    "    hashtags.append(tags)\n",
    "\n",
    "  pre_corpus = np.asarray(pre_corpus)\n",
    "\n",
    "  return pre_corpus\n",
    "\n",
    "\n",
    "def get_char_dict():\n",
    "  #creates a dictionary of characters and assigns a unique numerical index to\n",
    "  #each\n",
    "  #leaves index 0 empty to be used for padding\n",
    "\n",
    "  chars = string.digits\n",
    "  chars += string.ascii_lowercase\n",
    "  chars += \" \"\n",
    "  chars += string.punctuation\n",
    "  chars += '’'\n",
    "  chars += '‘'\n",
    "  chars = split(chars)\n",
    "  #add freq emoji unicode to arr\n",
    "  #for i in get_freq_emojis():\n",
    "  #  chars.append(i)\n",
    "  #add tag for other emoji to arr\n",
    "  #chars.append(other_emoji_tag)\n",
    "  chars.append(emoji_tag)\n",
    "  chars.append(user_tag)\n",
    "  #chars.append(new_line_tag)\n",
    "  \n",
    "  for i in range(len(chars)):\n",
    "    char_dict[chars[i]] = i+1\n",
    "\n",
    "  return char_dict\n",
    "\n",
    "\n",
    "def string_to_ids(input_string):\n",
    "  #converts a single string into a list of ids from the char_dict\n",
    "  \n",
    "  char_dict = get_char_dict()\n",
    "  string_ids = []\n",
    "  for char in input_string:\n",
    "    string_ids.append(char_dict[char])\n",
    "\n",
    "  return string_ids\n",
    "\n",
    "\n",
    "def corpus_to_ids(corpus):\n",
    "  #converts an entire corpus of strings into a list of ids from the char_dict\n",
    "  print(\"Converting corpus to ids\")\n",
    "  corp_arr = []\n",
    "  for string in corpus:\n",
    "    corp_arr.append(string_to_ids(string))\n",
    "\n",
    "  return corp_arr\n",
    "  \n",
    "\n",
    "def get_vocab_size():\n",
    "  #returns integer containing size of the character dictionary\n",
    "\n",
    "  return len(set(char_dict))\n",
    "\n",
    "\n",
    "def tag_dictionary(tags):\n",
    "\n",
    "  d = {ni: indi for indi, ni in enumerate(set(tags))}\n",
    "  numbers = [d[ni] for ni in tags]\n",
    "\n",
    "  return numbers\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3040,
     "status": "ok",
     "timestamp": 1598262120523,
     "user": {
      "displayName": "Joe Netherton",
      "photoUrl": "",
      "userId": "04700075469670085746"
     },
     "user_tz": -60
    },
    "id": "nYV1gVNPljUL",
    "outputId": "efd4b0ab-da18-473f-8368-ff906a1d913e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-aeb622b157f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Affiliation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-aeb622b157f5>\u001b[0m in \u001b[0;36mcsv_to_list\u001b[1;34m(data, field)\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m   \u001b[0mflat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mflat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#Convert data from csv file [\"Text\"] field to list\n",
    "\n",
    "def csv_to_list(data, field):\n",
    "\n",
    "  df = pd.DataFrame(data, columns= [field])\n",
    "  text = df.values.tolist()\n",
    "  flat = np.asarray(text).flatten()\n",
    "  return flat.tolist()\n",
    "\n",
    "\n",
    "\n",
    "corpus = csv_to_list(data, \"Text\")\n",
    "labels = csv_to_list(data, \"Affiliation\")\n",
    "\n",
    "corpus += csv_to_list(data2, \"Text\")\n",
    "labels += csv_to_list(data2, \"Affiliation\") \n",
    "if dataset == \"large\":\n",
    "  corpus += csv_to_list(data3, \"Text\")\n",
    "  labels += csv_to_list(data3, \"Affiliation\") \n",
    "\n",
    "print(\"Data contains\", len(corpus), \"samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1974,
     "status": "ok",
     "timestamp": 1598262125097,
     "user": {
      "displayName": "Joe Netherton",
      "photoUrl": "",
      "userId": "04700075469670085746"
     },
     "user_tz": -60
    },
    "id": "kMPIdZEkLsKn",
    "outputId": "f4ddb3cb-3faf-441f-96af-c2cd0ff08bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using topic dataset\n"
     ]
    }
   ],
   "source": [
    "#get topic dataset\n",
    "topic_corpus = []\n",
    "topic_labels = []\n",
    "\n",
    "\n",
    "if creating_topic_dataset == True:\n",
    "  for i in range(len(corpus)):\n",
    "\n",
    "    tweet = corpus[i]\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.split()\n",
    "\n",
    "    if any(check in keywords for check in tweet):\n",
    "      topic_corpus.append(corpus[i])\n",
    "      topic_labels.append(labels[i])\n",
    "      \n",
    "\n",
    "  print(len(topic_corpus))\n",
    "\n",
    "\n",
    "  corpus = topic_corpus\n",
    "  labels = topic_labels\n",
    "\n",
    "else:\n",
    "  print(\"Not using topic dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1598262126675,
     "user": {
      "displayName": "Joe Netherton",
      "photoUrl": "",
      "userId": "04700075469670085746"
     },
     "user_tz": -60
    },
    "id": "00G9aO6gfM1e",
    "outputId": "31adb268-90ae-472f-9405-9bcdca47fdfd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-72287c21cd27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28516,
     "status": "ok",
     "timestamp": 1598262155932,
     "user": {
      "displayName": "Joe Netherton",
      "photoUrl": "",
      "userId": "04700075469670085746"
     },
     "user_tz": -60
    },
    "id": "YuIIkTw9kvAd",
    "outputId": "97195ccf-2897-4456-8253-76763e4affbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing corpus . . .\n",
      "Converting corpus to ids\n"
     ]
    }
   ],
   "source": [
    "#Preprocess data\n",
    "tweets = preprocess_corpus(corpus)\n",
    "\n",
    "#Define X Data\n",
    "x = corpus_to_ids(tweets) #Vectorise tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1598262174301,
     "user": {
      "displayName": "Joe Netherton",
      "photoUrl": "",
      "userId": "04700075469670085746"
     },
     "user_tz": -60
    },
    "id": "72t5D0wSnJLp",
    "outputId": "53609326-d94a-4ef5-cca8-0a4659a149df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4940\n"
     ]
    }
   ],
   "source": [
    "#Pad sequences\n",
    "\n",
    "x = keras.preprocessing.sequence.pad_sequences(x, maxlen = MAX_LENGTH,\n",
    "                                                     padding='post')\n",
    "\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4OVTupqxXl1"
   },
   "outputs": [],
   "source": [
    "#Define y data\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "#tags = tags.ravel()\n",
    "\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "#y = encoded_labels\n",
    "y = tf.keras.utils.to_categorical(encoded_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnXQYXSCnX1Q"
   },
   "outputs": [],
   "source": [
    "#Segment Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if creating_topic_dataset == False:\n",
    "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "  print(x_test.shape)\n",
    "  print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1598968324965,
     "user": {
      "displayName": "Joe Netherton",
      "photoUrl": "",
      "userId": "04700075469670085746"
     },
     "user_tz": -60
    },
    "id": "q59O8fb_7na3",
    "outputId": "3a33902d-1ecd-4842-f9e8-658294026f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/thesis_files\n"
     ]
    }
   ],
   "source": [
    "#move to back to home directory\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRo4sNRsxiEG"
   },
   "outputs": [],
   "source": [
    "#Save preporcessed data in .npy file to the preprocessed_data folder in directory\n",
    "\n",
    "if creating_topic_dataset == True:\n",
    "  np.save(\"preprocessed_data/x_brexit.npy\", x)\n",
    "  np.save(\"preprocessed_data/y_brexit.npy\", y)\n",
    "\n",
    "\n",
    "elif dataset == \"large\":\n",
    "  np.save(\"preprocessed_data/x_train_large.npy\", x_train)\n",
    "  np.save(\"preprocessed_data/y_train_large.npy\", y_train)\n",
    "  np.save(\"preprocessed_data/x_test_large.npy\", x_test)\n",
    "  np.save(\"preprocessed_data/y_test_large.npy\", y_test)\n",
    "\n",
    "elif dataset == \"small\":\n",
    "  np.save(\"preprocessed_data/x_train_small.npy\", x_train)\n",
    "  np.save(\"preprocessed_data/y_train_small.npy\", y_train)\n",
    "  np.save(\"preprocessed_data/x_test_small.npy\", x_test)\n",
    "  np.save(\"preprocessed_data/y_test_small.npy\", y_test)\n",
    "\n",
    "\n",
    "print(\"Data Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UMmx1_M8ogS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBViihNPShGsY7vTF/8FsZ",
   "collapsed_sections": [],
   "name": "Tweet Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
